{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformations_casptone.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNn/PhqHfioRau2HKemQ4zH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MauricioFBL/capstone_project/blob/main/notebooks/transformations_casptone_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "TKVj2Uyd7Lt3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "#!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.2/spark-3.2.2-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.2-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "#os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.2-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession \n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "\n",
        "spark = SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"Our First Spark example\") \\\n",
        "       .getOrCreate()\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "#os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.2-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession \n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "\n",
        "spark = SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"Our First Spark example\") \\\n",
        "       .getOrCreate()\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "id": "JFgWln01TNb4",
        "outputId": "086fd59d-1827-4957-e38a-ec4cc82aa26c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f4290bff510>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://e49265cc8fdb:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Our First Spark example</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
        "from pyspark.sql.functions import *"
      ],
      "metadata": {
        "id": "-043QK6FGZfW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movie_review_df = (spark\n",
        "    .read\n",
        "    .option(\"header\", \"true\")\n",
        "    .option(\"inferSchema\", \"true\")\n",
        "    .csv('movie_review.csv'))"
      ],
      "metadata": {
        "id": "9Vz_t_XH91r2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movie_review_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyxPG91v91u9",
        "outputId": "f19877a8-d86c-4445-f400-3e66847a33e5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+---------+\n",
            "|  cid|          review_str|id_review|\n",
            "+-----+--------------------+---------+\n",
            "|13756|Once again Mr. Co...|        1|\n",
            "|15738|This is an exampl...|        2|\n",
            "|15727|First of all I ha...|        3|\n",
            "|17954|Not even the Beat...|        4|\n",
            "|16579|Brass pictures (m...|        5|\n",
            "+-----+--------------------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(inputCol='review_str', outputCol='Words')\n",
        "movie_review_df = tokenizer.transform(movie_review_df)\n",
        "movie_review_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PySkN8fBaIE",
        "outputId": "f54435eb-248a-4366-d0cc-aa4154deb242"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+---------+--------------------+\n",
            "|  cid|          review_str|id_review|               Words|\n",
            "+-----+--------------------+---------+--------------------+\n",
            "|13756|Once again Mr. Co...|        1|[once, again, mr....|\n",
            "|15738|This is an exampl...|        2|[this, is, an, ex...|\n",
            "|15727|First of all I ha...|        3|[first, of, all, ...|\n",
            "|17954|Not even the Beat...|        4|[not, even, the, ...|\n",
            "|16579|Brass pictures (m...|        5|[brass, pictures,...|\n",
            "+-----+--------------------+---------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "remover = StopWordsRemover(inputCol='Words', outputCol='words_filtered')\n",
        "movie_review_df = remover.transform(movie_review_df)\n",
        "movie_review_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CQ9NznpBaNo",
        "outputId": "d3ac7c19-b2f1-4e4e-849b-fd41223acb97"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+---------+--------------------+--------------------+\n",
            "|  cid|          review_str|id_review|               Words|      words_filtered|\n",
            "+-----+--------------------+---------+--------------------+--------------------+\n",
            "|13756|Once again Mr. Co...|        1|[once, again, mr....|[mr., costner, dr...|\n",
            "|15738|This is an exampl...|        2|[this, is, an, ex...|[example, majorit...|\n",
            "|15727|First of all I ha...|        3|[first, of, all, ...|[first, hate, mor...|\n",
            "|17954|Not even the Beat...|        4|[not, even, the, ...|[even, beatles, w...|\n",
            "|16579|Brass pictures (m...|        5|[brass, pictures,...|[brass, pictures,...|\n",
            "+-----+--------------------+---------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "movie_review_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DospRWnWBaqp",
        "outputId": "3b4b3fa2-6f6b-44be-848b-e4a148e5ee25"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- cid: integer (nullable = true)\n",
            " |-- review_str: string (nullable = true)\n",
            " |-- id_review: integer (nullable = true)\n",
            " |-- Words: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- words_filtered: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "os2fbU1mJGEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movie_review_df = movie_review_df.withColumn(\"positive_review\",when(array_contains(col(\"words_filtered\"),\"good\"),1).otherwise(0))\n",
        "movie_review_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGyvFNfKBauU",
        "outputId": "e4652ee4-6056-4d5e-9f8c-02e8c4370fed"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+---------+--------------------+--------------------+---------------+\n",
            "|  cid|          review_str|id_review|               Words|      words_filtered|positive_review|\n",
            "+-----+--------------------+---------+--------------------+--------------------+---------------+\n",
            "|13756|Once again Mr. Co...|        1|[once, again, mr....|[mr., costner, dr...|              0|\n",
            "|15738|This is an exampl...|        2|[this, is, an, ex...|[example, majorit...|              0|\n",
            "|15727|First of all I ha...|        3|[first, of, all, ...|[first, hate, mor...|              0|\n",
            "|17954|Not even the Beat...|        4|[not, even, the, ...|[even, beatles, w...|              0|\n",
            "|16579|Brass pictures (m...|        5|[brass, pictures,...|[brass, pictures,...|              1|\n",
            "+-----+--------------------+---------+--------------------+--------------------+---------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dt.datetime.now()\n",
        "movie_review_df = movie_review_df.withColumn('insert_date ', current_timestamp())\n",
        "movie_review_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piIzFDmOKj_-",
        "outputId": "872fcb09-edb6-4833-f62f-234df5ca1690"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+---------+--------------------+--------------------+---------------+--------------------+\n",
            "|  cid|          review_str|id_review|               Words|      words_filtered|positive_review|        insert_date |\n",
            "+-----+--------------------+---------+--------------------+--------------------+---------------+--------------------+\n",
            "|13756|Once again Mr. Co...|        1|[once, again, mr....|[mr., costner, dr...|              0|2022-07-28 00:31:...|\n",
            "|15738|This is an exampl...|        2|[this, is, an, ex...|[example, majorit...|              0|2022-07-28 00:31:...|\n",
            "|15727|First of all I ha...|        3|[first, of, all, ...|[first, hate, mor...|              0|2022-07-28 00:31:...|\n",
            "|17954|Not even the Beat...|        4|[not, even, the, ...|[even, beatles, w...|              0|2022-07-28 00:31:...|\n",
            "|16579|Brass pictures (m...|        5|[brass, pictures,...|[brass, pictures,...|              1|2022-07-28 00:31:...|\n",
            "+-----+--------------------+---------+--------------------+--------------------+---------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fg2Qk8L3KXLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "moviereviews = movie_review_df.drop('review_str','Words','words_filtered')"
      ],
      "metadata": {
        "id": "C5tWbD7mKR4Y"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "moviereviews.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdF97fhkLuEb",
        "outputId": "7d589eea-27a4-4d96-f1ed-1d982012cee8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------+---------------+--------------------+\n",
            "|  cid|id_review|positive_review|        insert_date |\n",
            "+-----+---------+---------------+--------------------+\n",
            "|13756|        1|              0|2022-07-28 00:31:...|\n",
            "|15738|        2|              0|2022-07-28 00:31:...|\n",
            "|15727|        3|              0|2022-07-28 00:31:...|\n",
            "|17954|        4|              0|2022-07-28 00:31:...|\n",
            "|16579|        5|              1|2022-07-28 00:31:...|\n",
            "+-----+---------+---------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "moviereviews.write.mode('overwrite').csv('clean_movie_revies.csv')"
      ],
      "metadata": {
        "id": "WvLVLFmjknFN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Log transformations"
      ],
      "metadata": {
        "id": "e4PNGWmGKCQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_review_df = (spark\n",
        "    .read\n",
        "    .option(\"header\", \"true\")\n",
        "    .option(\"inferSchema\", \"true\")\n",
        "    .csv('log_reviews.csv'))\n",
        "log_review_df.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzMCif6FS5Tm",
        "outputId": "cd5de050-ab7c-4b30-ef23-a1f8a24c38b0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------------+\n",
            "|id_review|                 log|\n",
            "+---------+--------------------+\n",
            "|        1|<reviewlog><log><...|\n",
            "|        2|<reviewlog><log><...|\n",
            "|        3|<reviewlog><log><...|\n",
            "|        4|<reviewlog><log><...|\n",
            "|        5|<reviewlog><log><...|\n",
            "|        6|<reviewlog><log><...|\n",
            "|        7|<reviewlog><log><...|\n",
            "|        8|<reviewlog><log><...|\n",
            "|        9|<reviewlog><log><...|\n",
            "|       10|<reviewlog><log><...|\n",
            "+---------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log_review_df = log_review_df.withColumn('log', regexp_replace(\n",
        "    'log', '<reviewlog><log><logDate>', '')).withColumn('log', regexp_replace(\n",
        "    'log', '</phoneNumber></log></reviewlog>', ''))\n",
        "log_review_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pg6GXSGpcJD",
        "outputId": "5ea49337-3b50-4f3f-ddaa-de7db9af8fb0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------------+\n",
            "|id_review|                 log|\n",
            "+---------+--------------------+\n",
            "|        1|04-25-2021</logDa...|\n",
            "|        2|03-13-2021</logDa...|\n",
            "|        3|09-30-2021</logDa...|\n",
            "|        4|05-24-2021</logDa...|\n",
            "|        5|02-01-2021</logDa...|\n",
            "+---------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log_review_df = log_review_df.withColumn(\n",
        "    'log_date', split(log_review_df['log'], \n",
        "                      '</logDate><device>').getItem(0))\n",
        "log_review_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWu2ij5Uufhm",
        "outputId": "629688ae-9c72-4277-8574-40d9439ee8c5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------------+----------+\n",
            "|id_review|                 log|  log_date|\n",
            "+---------+--------------------+----------+\n",
            "|        1|04-25-2021</logDa...|04-25-2021|\n",
            "|        2|03-13-2021</logDa...|03-13-2021|\n",
            "|        3|09-30-2021</logDa...|09-30-2021|\n",
            "|        4|05-24-2021</logDa...|05-24-2021|\n",
            "|        5|02-01-2021</logDa...|02-01-2021|\n",
            "+---------+--------------------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log_review_df = log_review_df.withColumn(\n",
        "    'device_drop', split(log_review_df['log'], \n",
        "                      '</device><location>').getItem(0)\n",
        "    )\n",
        "log_review_df = log_review_df.withColumn(\n",
        "    'device', split(log_review_df['device_drop'], \n",
        "                      '</logDate><device>').getItem(1)\n",
        "    )\n",
        "\n",
        "log_review_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ane1AgDuflk",
        "outputId": "b6581c5d-338f-44ea-e2e6-fc6701557b13"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------------+----------+--------------------+------+\n",
            "|id_review|                 log|  log_date|         device_drop|device|\n",
            "+---------+--------------------+----------+--------------------+------+\n",
            "|        1|04-25-2021</logDa...|04-25-2021|04-25-2021</logDa...|Mobile|\n",
            "|        2|03-13-2021</logDa...|03-13-2021|03-13-2021</logDa...|Tablet|\n",
            "|        3|09-30-2021</logDa...|09-30-2021|09-30-2021</logDa...|Tablet|\n",
            "|        4|05-24-2021</logDa...|05-24-2021|05-24-2021</logDa...|Tablet|\n",
            "|        5|02-01-2021</logDa...|02-01-2021|02-01-2021</logDa...|Tablet|\n",
            "+---------+--------------------+----------+--------------------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log_review_df = log_review_df.withColumn(\n",
        "    'device_drop', split(log_review_df['log'], \n",
        "                      '</location><os>').getItem(0)\n",
        "    )\n",
        "log_review_df = log_review_df.withColumn(\n",
        "    'location', split(log_review_df['device_drop'], \n",
        "                      '</device><location>').getItem(1)\n",
        "    )\n",
        "\n",
        "log_review_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GYsNPstufrW",
        "outputId": "2d6439d8-9a13-40ec-9ae8-189be3494f76"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------------+----------+--------------------+------+-------------+\n",
            "|id_review|                 log|  log_date|         device_drop|device|     location|\n",
            "+---------+--------------------+----------+--------------------+------+-------------+\n",
            "|        1|04-25-2021</logDa...|04-25-2021|04-25-2021</logDa...|Mobile|       Kansas|\n",
            "|        2|03-13-2021</logDa...|03-13-2021|03-13-2021</logDa...|Tablet|       Oregon|\n",
            "|        3|09-30-2021</logDa...|09-30-2021|09-30-2021</logDa...|Tablet|    Minnesota|\n",
            "|        4|05-24-2021</logDa...|05-24-2021|05-24-2021</logDa...|Tablet|     Arkansas|\n",
            "|        5|02-01-2021</logDa...|02-01-2021|02-01-2021</logDa...|Tablet|New Hampshire|\n",
            "+---------+--------------------+----------+--------------------+------+-------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log_review_df = log_review_df.withColumn(\n",
        "    'device_drop', split(log_review_df['log'], \n",
        "                      '</os><ipAddress>').getItem(0)\n",
        "    )\n",
        "log_review_df = log_review_df.withColumn(\n",
        "    'os', split(log_review_df['device_drop'], \n",
        "                      '</location><os>').getItem(1)\n",
        "    )\n",
        "\n",
        "log_review_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNsVnImdyFwS",
        "outputId": "6193ee03-483a-4e30-e4da-2b4989eabdfb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------------+----------+--------------------+------+-------------+--------------+\n",
            "|id_review|                 log|  log_date|         device_drop|device|     location|            os|\n",
            "+---------+--------------------+----------+--------------------+------+-------------+--------------+\n",
            "|        1|04-25-2021</logDa...|04-25-2021|04-25-2021</logDa...|Mobile|       Kansas|     Apple iOS|\n",
            "|        2|03-13-2021</logDa...|03-13-2021|03-13-2021</logDa...|Tablet|       Oregon|Google Android|\n",
            "|        3|09-30-2021</logDa...|09-30-2021|09-30-2021</logDa...|Tablet|    Minnesota|     Apple iOS|\n",
            "|        4|05-24-2021</logDa...|05-24-2021|05-24-2021</logDa...|Tablet|     Arkansas|   Apple MacOS|\n",
            "|        5|02-01-2021</logDa...|02-01-2021|02-01-2021</logDa...|Tablet|New Hampshire|         Linux|\n",
            "+---------+--------------------+----------+--------------------+------+-------------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log_review_df = log_review_df.withColumn(\n",
        "    'phoneNumber', split(log_review_df['log'], \n",
        "                      '</ipAddress><phoneNumber>').getItem(1)\n",
        "    )\n",
        "# log_review_df = log_review_df.withColumn(\n",
        "#     'ipAddress', split(log_review_df['device_drop'], \n",
        "#                       '</os><ipAddress>').getItem(1)\n",
        "#     )\n",
        "\n",
        "log_review_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3590DiqPzilT",
        "outputId": "e188d30e-60aa-4b33-d75f-09d94ff18317"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------------+----------+--------------------+------+-------------+--------------+------------+\n",
            "|id_review|                 log|  log_date|         device_drop|device|     location|            os| phoneNumber|\n",
            "+---------+--------------------+----------+--------------------+------+-------------+--------------+------------+\n",
            "|        1|04-25-2021</logDa...|04-25-2021|04-25-2021</logDa...|Mobile|       Kansas|     Apple iOS|821-540-5777|\n",
            "|        2|03-13-2021</logDa...|03-13-2021|03-13-2021</logDa...|Tablet|       Oregon|Google Android|819-102-1320|\n",
            "|        3|09-30-2021</logDa...|09-30-2021|09-30-2021</logDa...|Tablet|    Minnesota|     Apple iOS|989-156-0498|\n",
            "|        4|05-24-2021</logDa...|05-24-2021|05-24-2021</logDa...|Tablet|     Arkansas|   Apple MacOS|225-837-9935|\n",
            "|        5|02-01-2021</logDa...|02-01-2021|02-01-2021</logDa...|Tablet|New Hampshire|         Linux|243-842-4562|\n",
            "+---------+--------------------+----------+--------------------+------+-------------+--------------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GJz2q5XUziaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#____________________________________"
      ],
      "metadata": {
        "id": "sSXn7LEplOte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sourceDf = spark.read.csv('log_reviews.csv', sep=',', header=True, inferSchema=True, multiLine=True)\n",
        "sourceDf.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jinFiiPRBayO",
        "outputId": "5a55615f-1f11-4487-ffc0-1259534811d4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------------+\n",
            "|id_review|                 log|\n",
            "+---------+--------------------+\n",
            "|        1|<reviewlog><log><...|\n",
            "|        2|<reviewlog><log><...|\n",
            "|        3|<reviewlog><log><...|\n",
            "|        4|<reviewlog><log><...|\n",
            "|        5|<reviewlog><log><...|\n",
            "+---------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# xmlfile = sourceDf.select('log')\n",
        "# xmlfile.show(5)"
      ],
      "metadata": {
        "id": "Nwc1JwI_okqB"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sourceDf = sourceDf.withColumn('log', regexp_replace(\n",
        "    'log', '<reviewlog><log><logDate>', '')).withColumn('log', regexp_replace(\n",
        "    'log', '</phoneNumber></log></reviewlog>', ''))\n",
        "\n",
        "log_review_df_2 = sourceDf.withColumn('log', regexp_replace(\n",
        "    'log', '\\<(.*?)\\>', ';')).withColumn('log', regexp_replace(\n",
        "    'log', ';;', ';'))\n",
        "\n",
        "log_review_df_2 = log_review_df_2.withColumn(\n",
        "    'log_date', split(log_review_df_2['log'], \n",
        "                      ';').getItem(0))\n",
        "log_review_df_2 = log_review_df_2.withColumn(\n",
        "    'device', split(log_review_df_2['log'], \n",
        "                      ';').getItem(1))\n",
        "log_review_df_2 = log_review_df_2.withColumn(\n",
        "    'location', split(log_review_df_2['log'], \n",
        "                      ';').getItem(2))\n",
        "log_review_df_2 = log_review_df_2.withColumn(\n",
        "    'os', split(log_review_df_2['log'], \n",
        "                      ';').getItem(3))\n",
        "log_review_df_2 = log_review_df_2.withColumn(\n",
        "    'ip_address', split(log_review_df_2['log'], \n",
        "                      ';').getItem(4))\n",
        "log_review_df_2 = log_review_df_2.withColumn(\n",
        "    'phoneNumber', split(log_review_df_2['log'], \n",
        "                      ';').getItem(5))\n",
        "\n",
        "\n",
        "log_review_df_2.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozwjM3tHyFtO",
        "outputId": "2449bac2-94bb-45fa-d744-a7ebf7c5438e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------------+----------+------+-------------+--------------+------------+------------+\n",
            "|id_review|                 log|  log_date|device|     location|            os|  ip_address| phoneNumber|\n",
            "+---------+--------------------+----------+------+-------------+--------------+------------+------------+\n",
            "|        1|04-25-2021;Mobile...|04-25-2021|Mobile|       Kansas|     Apple iOS|9.200.232.57|821-540-5777|\n",
            "|        2|03-13-2021;Tablet...|03-13-2021|Tablet|       Oregon|Google Android|9.200.232.57|819-102-1320|\n",
            "|        3|09-30-2021;Tablet...|09-30-2021|Tablet|    Minnesota|     Apple iOS|9.200.232.57|989-156-0498|\n",
            "|        4|05-24-2021;Tablet...|05-24-2021|Tablet|     Arkansas|   Apple MacOS|9.200.232.57|225-837-9935|\n",
            "|        5|02-01-2021;Tablet...|02-01-2021|Tablet|New Hampshire|         Linux|9.200.232.57|243-842-4562|\n",
            "+---------+--------------------+----------+------+-------------+--------------+------------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log_review_df_2 = log_review_df_2.drop('log')"
      ],
      "metadata": {
        "id": "Vv6RsUi2ufzC"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_review_df_2.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B19r2WZRpCSq",
        "outputId": "fde035b4-59bb-4f1f-bd1a-7d4fa84890fa"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+------+-------------+--------------+------------+------------+\n",
            "|id_review|  log_date|device|     location|            os|  ip_address| phoneNumber|\n",
            "+---------+----------+------+-------------+--------------+------------+------------+\n",
            "|        1|04-25-2021|Mobile|       Kansas|     Apple iOS|9.200.232.57|821-540-5777|\n",
            "|        2|03-13-2021|Tablet|       Oregon|Google Android|9.200.232.57|819-102-1320|\n",
            "|        3|09-30-2021|Tablet|    Minnesota|     Apple iOS|9.200.232.57|989-156-0498|\n",
            "|        4|05-24-2021|Tablet|     Arkansas|   Apple MacOS|9.200.232.57|225-837-9935|\n",
            "|        5|02-01-2021|Tablet|New Hampshire|         Linux|9.200.232.57|243-842-4562|\n",
            "+---------+----------+------+-------------+--------------+------------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log_review_df_2.write.mode('overwrite').csv('clean_log_reviews.csv')"
      ],
      "metadata": {
        "id": "qSXg8oVDoqKM"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ____________________________________________________________"
      ],
      "metadata": {
        "id": "9BmKt1kNlOkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#   import com.databricks.spark.xml._\n",
        "#   import com.databricks.spark.xml.functions.from_xml\n",
        "\n",
        "#   val spark = SparkSession.builder().master(\"local[*]\").getOrCreate()\n",
        "#   import spark.implicits._\n",
        "#   spark.sparkContext.setLogLevel(\"ERROR\")\n",
        "\n",
        "#   val df = // Read csv file\n",
        "  \n",
        "#   // Assuming your xml content column name is xmldata\n",
        "#   val xmlSchema = schema_of_xml(df.select(\"xmldata\").as[String])\n",
        "\n",
        "#   df.withColumn(\"xmldata\", from_xml('xmldata, xmlSchema))\n",
        "#     .select(\"*\", \"xmldata.ab\")\n",
        "#     .selectExpr(df.columns.diff(Array(\"xmldata\")) ++\n",
        "#       Array(\"ab[0]._a as name\", \"ab[0]._b as id\", \"ab[1]._a as manager_name\", \"ab[1]._b as manager_id\"): _*)\n",
        "#     .show(false)"
      ],
      "metadata": {
        "id": "HaWxBN4EBa1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !$SPARK_HOME/bin/spark-shell --packages com.databricks:spark-xml_2.12:0.15.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wy8a9dHlPYCp",
        "outputId": "226083c6-c1a4-4cfc-ff3f-70934b240fc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.2-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "com.databricks#spark-xml_2.12 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c1fc4ee6-2798-4c49-83ff-ad94decf51a8;1.0\n",
            "\tconfs: [default]\n",
            "\tfound com.databricks#spark-xml_2.12;0.15.0 in central\n",
            "\tfound commons-io#commons-io;2.11.0 in central\n",
            "\tfound org.glassfish.jaxb#txw2;3.0.2 in central\n",
            "\tfound org.apache.ws.xmlschema#xmlschema-core;2.3.0 in central\n",
            "downloading https://repo1.maven.org/maven2/com/databricks/spark-xml_2.12/0.15.0/spark-xml_2.12-0.15.0.jar ...\n",
            "\t[SUCCESSFUL ] com.databricks#spark-xml_2.12;0.15.0!spark-xml_2.12.jar (44ms)\n",
            "downloading https://repo1.maven.org/maven2/commons-io/commons-io/2.11.0/commons-io-2.11.0.jar ...\n",
            "\t[SUCCESSFUL ] commons-io#commons-io;2.11.0!commons-io.jar (38ms)\n",
            "downloading https://repo1.maven.org/maven2/org/glassfish/jaxb/txw2/3.0.2/txw2-3.0.2.jar ...\n",
            "\t[SUCCESSFUL ] org.glassfish.jaxb#txw2;3.0.2!txw2.jar (22ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/ws/xmlschema/xmlschema-core/2.3.0/xmlschema-core-2.3.0.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.ws.xmlschema#xmlschema-core;2.3.0!xmlschema-core.jar(bundle) (29ms)\n",
            ":: resolution report :: resolve 4942ms :: artifacts dl 153ms\n",
            "\t:: modules in use:\n",
            "\tcom.databricks#spark-xml_2.12;0.15.0 from central in [default]\n",
            "\tcommons-io#commons-io;2.11.0 from central in [default]\n",
            "\torg.apache.ws.xmlschema#xmlschema-core;2.3.0 from central in [default]\n",
            "\torg.glassfish.jaxb#txw2;3.0.2 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   4   |   4   |   4   |   0   ||   4   |   4   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-c1fc4ee6-2798-4c49-83ff-ad94decf51a8\n",
            "\tconfs: [default]\n",
            "\t4 artifacts copied, 0 already retrieved (717kB/35ms)\n",
            "22/07/23 18:08:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "22/07/23 18:09:01 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "Spark context Web UI available at http://f6fccfaaac10:4041\n",
            "Spark context available as 'sc' (master = local[*], app id = local-1658599741630).\n",
            "Spark session available as 'spark'.\n",
            "Welcome to\n",
            "      ____              __\n",
            "     / __/__  ___ _____/ /__\n",
            "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
            "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.2.2\n",
            "      /_/\n",
            "         \n",
            "Using Scala version 2.12.15 (OpenJDK 64-Bit Server VM, Java 1.8.0_312)\n",
            "Type in expressions to have them evaluated.\n",
            "Type :help for more information.\n",
            "\u001b[35m\n",
            "scala> \u001b[0m\n",
            "\u001b[35m\n",
            "scala> \u001b[0m\n",
            "\u001b[35m\n",
            "scala> \u001b[0mexit()\n",
            "<console>:23: \u001b[31merror: \u001b[0mnot found: value exit\n",
            "       exit()\n",
            "       ^\n",
            "\u001b[35m\n",
            "scala> \u001b[0mexit\n",
            "<console>:23: \u001b[31merror: \u001b[0mnot found: value exit\n",
            "       exit\n",
            "       ^\n",
            "\u001b[35m\n",
            "scala> \u001b[0m:q\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.12:0.12.0 pyspark-shell'"
      ],
      "metadata": {
        "id": "pIqyYu3ARRYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import databricks.spark.xml\n",
        "# import databricks.spark.xml.functions.from_xml\n",
        "# from pyspark"
      ],
      "metadata": {
        "id": "CuwLTlx0Ba4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.types import *"
      ],
      "metadata": {
        "id": "Wk6_NDd1SLCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyspark.sql import SparkSession\n",
        "# from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "# from pyspark.sql.functions import *\n",
        "# from decimal import Decimal\n",
        "# appName = \"Python Example - PySpark Read XML\"\n",
        "# master = \"local\"\n",
        "\n",
        "# # Create Spark session\n",
        "# spark_2 = SparkSession.builder \\\n",
        "#     .appName(appName) \\\n",
        "#     .master(master) \\\n",
        "#     .config(\"spark.jars.packages\", \"com.databricks:spark-xml_2.11:0.9.0\") \\\n",
        "#     .getOrCreate()"
      ],
      "metadata": {
        "id": "n8wDa4hJS5Zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customSchema = StructType([\n",
        "    StructField(\"logDate\", StringType(), True),\n",
        "    StructField(\"device\", StringType(), True),\n",
        "    StructField(\"location\", StringType(), True),\n",
        "    StructField(\"os\", StringType(), True),\n",
        "    StructField(\"ipAddress\", StringType(), True),\n",
        "    StructField(\"phoneNumber\", StringType(), True)])"
      ],
      "metadata": {
        "id": "3oEKjdiglxsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_review_df_2.write.format('csv').save('mycsv.csv')"
      ],
      "metadata": {
        "id": "agzn92uVp_9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# xmlSchema = spark.read.format('xml').options(rowTag='log').load(log_review_df.select(\"log\"))\n",
        "df = (spark.read.format(\"com.databricks.spark.xml\"\n",
        "              ).option(\"rowTag\",\"log\"\n",
        "              ).load(log_review_df.select(\"log\"), \n",
        "                     schema=customSchema))"
      ],
      "metadata": {
        "id": "tkFd8p5Vp44X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0fVHe6YVS5Gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_review_df = (spark\n",
        "    .read\n",
        "    .option(\"header\", \"true\")\n",
        "    .option(\"inferSchema\", \"true\")\n",
        "    .csv('log_reviews.csv'))\n",
        "log_review_df.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84b3a103-84c6-49f2-f47a-647ef1cdeed1",
        "id": "x99tkGVIWOh5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------------+\n",
            "|id_review|                 log|\n",
            "+---------+--------------------+\n",
            "|        1|<reviewlog><log><...|\n",
            "|        2|<reviewlog><log><...|\n",
            "|        3|<reviewlog><log><...|\n",
            "|        4|<reviewlog><log><...|\n",
            "|        5|<reviewlog><log><...|\n",
            "|        6|<reviewlog><log><...|\n",
            "|        7|<reviewlog><log><...|\n",
            "|        8|<reviewlog><log><...|\n",
            "|        9|<reviewlog><log><...|\n",
            "|       10|<reviewlog><log><...|\n",
            "+---------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#   df.withColumn(\"xmldata\", from_xml('xmldata, xmlSchema))\n",
        "#     .select(\"*\", \"xmldata.ab\")\n",
        "#     .selectExpr(df.columns.diff(Array(\"xmldata\")) ++\n",
        "#       Array(\"ab[0]._a as name\", \"ab[0]._b as id\", \"ab[1]._a as manager_name\", \"ab[1]._b as manager_id\"): _*)\n",
        "#     .show(false)"
      ],
      "metadata": {
        "id": "ercOKyRKBa7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PKAq0DGwBa_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_purchase_df = (spark\n",
        "    .read\n",
        "    .option(\"header\", \"true\")\n",
        "    .option(\"inferSchema\", \"true\")\n",
        "    .csv('user_purchase.csv'))"
      ],
      "metadata": {
        "id": "uWz_ANBT905b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_purchase_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1P6cabs90-i",
        "outputId": "646be2fa-eb4e-4c9c-8a0a-08b5e99e7a26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
            "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
            "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
            "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
            "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
            "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n",
            "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
            "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
            "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc.install_pypi_package(\"com.databricks.spark.xml\")"
      ],
      "metadata": {
        "id": "ZLdWg3IV91Dq",
        "outputId": "a2570a46-1d44-4869-8903-aca92e10abdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        }
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-b509cc9e9bba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstall_pypi_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"com.databricks.spark.xml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.2-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "rUdtxzJu91He"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ../content/spark-3.2.2-bin-hadoop3.2/jars"
      ],
      "metadata": {
        "id": "zOtTRITiOn--",
        "outputId": "9b26365c-bfc2-4a5d-963d-210e6a5ba92c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "activation-1.1.1.jar\n",
            "aircompressor-0.21.jar\n",
            "algebra_2.12-2.0.1.jar\n",
            "annotations-17.0.0.jar\n",
            "antlr4-runtime-4.8.jar\n",
            "antlr-runtime-3.5.2.jar\n",
            "aopalliance-repackaged-2.6.1.jar\n",
            "arpack-2.2.1.jar\n",
            "arpack_combined_all-0.1.jar\n",
            "arrow-format-2.0.0.jar\n",
            "arrow-memory-core-2.0.0.jar\n",
            "arrow-memory-netty-2.0.0.jar\n",
            "arrow-vector-2.0.0.jar\n",
            "audience-annotations-0.5.0.jar\n",
            "automaton-1.11-8.jar\n",
            "avro-1.10.2.jar\n",
            "avro-ipc-1.10.2.jar\n",
            "avro-mapred-1.10.2.jar\n",
            "blas-2.2.1.jar\n",
            "bonecp-0.8.0.RELEASE.jar\n",
            "breeze_2.12-1.2.jar\n",
            "breeze-macros_2.12-1.2.jar\n",
            "cats-kernel_2.12-2.1.1.jar\n",
            "chill_2.12-0.10.0.jar\n",
            "chill-java-0.10.0.jar\n",
            "commons-cli-1.2.jar\n",
            "commons-codec-1.15.jar\n",
            "commons-collections-3.2.2.jar\n",
            "commons-compiler-3.0.16.jar\n",
            "commons-compress-1.21.jar\n",
            "commons-crypto-1.1.0.jar\n",
            "commons-dbcp-1.4.jar\n",
            "commons-io-2.8.0.jar\n",
            "commons-lang-2.6.jar\n",
            "commons-lang3-3.12.0.jar\n",
            "commons-logging-1.1.3.jar\n",
            "commons-math3-3.4.1.jar\n",
            "commons-net-3.1.jar\n",
            "commons-pool-1.5.4.jar\n",
            "commons-text-1.6.jar\n",
            "compress-lzf-1.0.3.jar\n",
            "core-1.1.2.jar\n",
            "curator-client-2.13.0.jar\n",
            "curator-framework-2.13.0.jar\n",
            "curator-recipes-2.13.0.jar\n",
            "datanucleus-api-jdo-4.2.4.jar\n",
            "datanucleus-core-4.1.17.jar\n",
            "datanucleus-rdbms-4.1.19.jar\n",
            "derby-10.14.2.0.jar\n",
            "dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar\n",
            "flatbuffers-java-1.9.0.jar\n",
            "generex-1.0.2.jar\n",
            "gson-2.2.4.jar\n",
            "guava-14.0.1.jar\n",
            "hadoop-client-api-3.3.1.jar\n",
            "hadoop-client-runtime-3.3.1.jar\n",
            "hadoop-shaded-guava-1.1.1.jar\n",
            "hadoop-yarn-server-web-proxy-3.3.1.jar\n",
            "HikariCP-2.5.1.jar\n",
            "hive-beeline-2.3.9.jar\n",
            "hive-cli-2.3.9.jar\n",
            "hive-common-2.3.9.jar\n",
            "hive-exec-2.3.9-core.jar\n",
            "hive-jdbc-2.3.9.jar\n",
            "hive-llap-common-2.3.9.jar\n",
            "hive-metastore-2.3.9.jar\n",
            "hive-serde-2.3.9.jar\n",
            "hive-service-rpc-3.1.2.jar\n",
            "hive-shims-0.23-2.3.9.jar\n",
            "hive-shims-2.3.9.jar\n",
            "hive-shims-common-2.3.9.jar\n",
            "hive-shims-scheduler-2.3.9.jar\n",
            "hive-storage-api-2.7.2.jar\n",
            "hive-vector-code-gen-2.3.9.jar\n",
            "hk2-api-2.6.1.jar\n",
            "hk2-locator-2.6.1.jar\n",
            "hk2-utils-2.6.1.jar\n",
            "htrace-core4-4.1.0-incubating.jar\n",
            "httpclient-4.5.13.jar\n",
            "httpcore-4.4.14.jar\n",
            "istack-commons-runtime-3.0.8.jar\n",
            "ivy-2.5.0.jar\n",
            "jackson-annotations-2.12.3.jar\n",
            "jackson-core-2.12.3.jar\n",
            "jackson-core-asl-1.9.13.jar\n",
            "jackson-databind-2.12.3.jar\n",
            "jackson-dataformat-yaml-2.12.3.jar\n",
            "jackson-datatype-jsr310-2.11.2.jar\n",
            "jackson-mapper-asl-1.9.13.jar\n",
            "jackson-module-scala_2.12-2.12.3.jar\n",
            "jakarta.annotation-api-1.3.5.jar\n",
            "jakarta.inject-2.6.1.jar\n",
            "jakarta.servlet-api-4.0.3.jar\n",
            "jakarta.validation-api-2.0.2.jar\n",
            "jakarta.ws.rs-api-2.1.6.jar\n",
            "jakarta.xml.bind-api-2.3.2.jar\n",
            "janino-3.0.16.jar\n",
            "javassist-3.25.0-GA.jar\n",
            "javax.jdo-3.2.0-m3.jar\n",
            "javolution-5.5.1.jar\n",
            "jaxb-api-2.2.11.jar\n",
            "jaxb-runtime-2.3.2.jar\n",
            "jcl-over-slf4j-1.7.30.jar\n",
            "jdo-api-3.0.1.jar\n",
            "jersey-client-2.34.jar\n",
            "jersey-common-2.34.jar\n",
            "jersey-container-servlet-2.34.jar\n",
            "jersey-container-servlet-core-2.34.jar\n",
            "jersey-hk2-2.34.jar\n",
            "jersey-server-2.34.jar\n",
            "JLargeArrays-1.5.jar\n",
            "jline-2.14.6.jar\n",
            "joda-time-2.10.10.jar\n",
            "jodd-core-3.5.2.jar\n",
            "jpam-1.1.jar\n",
            "json-1.8.jar\n",
            "json4s-ast_2.12-3.7.0-M11.jar\n",
            "json4s-core_2.12-3.7.0-M11.jar\n",
            "json4s-jackson_2.12-3.7.0-M11.jar\n",
            "json4s-scalap_2.12-3.7.0-M11.jar\n",
            "jsr305-3.0.0.jar\n",
            "jta-1.1.jar\n",
            "JTransforms-3.1.jar\n",
            "jul-to-slf4j-1.7.30.jar\n",
            "kryo-shaded-4.0.2.jar\n",
            "kubernetes-client-5.4.1.jar\n",
            "kubernetes-model-admissionregistration-5.4.1.jar\n",
            "kubernetes-model-apiextensions-5.4.1.jar\n",
            "kubernetes-model-apps-5.4.1.jar\n",
            "kubernetes-model-autoscaling-5.4.1.jar\n",
            "kubernetes-model-batch-5.4.1.jar\n",
            "kubernetes-model-certificates-5.4.1.jar\n",
            "kubernetes-model-common-5.4.1.jar\n",
            "kubernetes-model-coordination-5.4.1.jar\n",
            "kubernetes-model-core-5.4.1.jar\n",
            "kubernetes-model-discovery-5.4.1.jar\n",
            "kubernetes-model-events-5.4.1.jar\n",
            "kubernetes-model-extensions-5.4.1.jar\n",
            "kubernetes-model-flowcontrol-5.4.1.jar\n",
            "kubernetes-model-metrics-5.4.1.jar\n",
            "kubernetes-model-networking-5.4.1.jar\n",
            "kubernetes-model-node-5.4.1.jar\n",
            "kubernetes-model-policy-5.4.1.jar\n",
            "kubernetes-model-rbac-5.4.1.jar\n",
            "kubernetes-model-scheduling-5.4.1.jar\n",
            "kubernetes-model-storageclass-5.4.1.jar\n",
            "lapack-2.2.1.jar\n",
            "leveldbjni-all-1.8.jar\n",
            "libfb303-0.9.3.jar\n",
            "libthrift-0.12.0.jar\n",
            "log4j-1.2.17.jar\n",
            "logging-interceptor-3.12.12.jar\n",
            "lz4-java-1.7.1.jar\n",
            "macro-compat_2.12-1.1.1.jar\n",
            "mesos-1.4.0-shaded-protobuf.jar\n",
            "metrics-core-4.2.0.jar\n",
            "metrics-graphite-4.2.0.jar\n",
            "metrics-jmx-4.2.0.jar\n",
            "metrics-json-4.2.0.jar\n",
            "metrics-jvm-4.2.0.jar\n",
            "minlog-1.3.0.jar\n",
            "netty-all-4.1.68.Final.jar\n",
            "objenesis-2.6.jar\n",
            "okhttp-3.12.12.jar\n",
            "okio-1.14.0.jar\n",
            "opencsv-2.3.jar\n",
            "orc-core-1.6.14.jar\n",
            "orc-mapreduce-1.6.14.jar\n",
            "orc-shims-1.6.14.jar\n",
            "oro-2.0.8.jar\n",
            "osgi-resource-locator-1.0.3.jar\n",
            "paranamer-2.8.jar\n",
            "parquet-column-1.12.2.jar\n",
            "parquet-common-1.12.2.jar\n",
            "parquet-encoding-1.12.2.jar\n",
            "parquet-format-structures-1.12.2.jar\n",
            "parquet-hadoop-1.12.2.jar\n",
            "parquet-jackson-1.12.2.jar\n",
            "protobuf-java-2.5.0.jar\n",
            "py4j-0.10.9.5.jar\n",
            "pyrolite-4.30.jar\n",
            "RoaringBitmap-0.9.0.jar\n",
            "rocksdbjni-6.20.3.jar\n",
            "scala-collection-compat_2.12-2.1.1.jar\n",
            "scala-compiler-2.12.15.jar\n",
            "scala-library-2.12.15.jar\n",
            "scala-parser-combinators_2.12-1.1.2.jar\n",
            "scala-reflect-2.12.15.jar\n",
            "scala-xml_2.12-1.2.0.jar\n",
            "shapeless_2.12-2.3.3.jar\n",
            "shims-0.9.0.jar\n",
            "slf4j-api-1.7.30.jar\n",
            "slf4j-log4j12-1.7.30.jar\n",
            "snakeyaml-1.27.jar\n",
            "snappy-java-1.1.8.4.jar\n",
            "spark-catalyst_2.12-3.2.2.jar\n",
            "spark-core_2.12-3.2.2.jar\n",
            "spark-graphx_2.12-3.2.2.jar\n",
            "spark-hive_2.12-3.2.2.jar\n",
            "spark-hive-thriftserver_2.12-3.2.2.jar\n",
            "spark-kubernetes_2.12-3.2.2.jar\n",
            "spark-kvstore_2.12-3.2.2.jar\n",
            "spark-launcher_2.12-3.2.2.jar\n",
            "spark-mesos_2.12-3.2.2.jar\n",
            "spark-mllib_2.12-3.2.2.jar\n",
            "spark-mllib-local_2.12-3.2.2.jar\n",
            "spark-network-common_2.12-3.2.2.jar\n",
            "spark-network-shuffle_2.12-3.2.2.jar\n",
            "spark-repl_2.12-3.2.2.jar\n",
            "spark-sketch_2.12-3.2.2.jar\n",
            "spark-sql_2.12-3.2.2.jar\n",
            "spark-streaming_2.12-3.2.2.jar\n",
            "spark-tags_2.12-3.2.2.jar\n",
            "spark-tags_2.12-3.2.2-tests.jar\n",
            "spark-unsafe_2.12-3.2.2.jar\n",
            "spark-xml_2.11-0.9.0.jar\n",
            "spark-yarn_2.12-3.2.2.jar\n",
            "spire_2.12-0.17.0.jar\n",
            "spire-macros_2.12-0.17.0.jar\n",
            "spire-platform_2.12-0.17.0.jar\n",
            "spire-util_2.12-0.17.0.jar\n",
            "ST4-4.0.4.jar\n",
            "stax-api-1.0.1.jar\n",
            "stream-2.9.6.jar\n",
            "super-csv-2.2.0.jar\n",
            "threeten-extra-1.5.0.jar\n",
            "tink-1.6.0.jar\n",
            "transaction-api-1.1.jar\n",
            "univocity-parsers-2.9.1.jar\n",
            "velocity-1.5.jar\n",
            "xbean-asm9-shaded-4.20.jar\n",
            "xz-1.8.jar\n",
            "zjsonpatch-0.3.0.jar\n",
            "zookeeper-3.6.2.jar\n",
            "zookeeper-jute-3.6.2.jar\n",
            "zstd-jni-1.5.0-4.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ../content/spark-3.2.2-bin-hadoop3.2/jars \n",
        "# sudo wget https://repo1.maven.org/maven2/com/databricks/spark-xml_2.11/0.9.0/spark-xml_2.11-0.9.0.jar"
      ],
      "metadata": {
        "id": "XZm86rRcO4ZT",
        "outputId": "03ee9522-695c-4b7a-a60f-5e30e8cd8a60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://repo1.maven.org/maven2/com/databricks/spark-xml_2.11/0.9.0/spark-xml_2.11-0.9.0.jar"
      ],
      "metadata": {
        "id": "k0AUeEWMO4Wz",
        "outputId": "fd4f2a44-1fca-42f9-a9e8-e9ebe0288dd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-07-28 00:51:14--  https://repo1.maven.org/maven2/com/databricks/spark-xml_2.11/0.9.0/spark-xml_2.11-0.9.0.jar\n",
            "Resolving repo1.maven.org (repo1.maven.org)... 199.232.192.209, 199.232.196.209\n",
            "Connecting to repo1.maven.org (repo1.maven.org)|199.232.192.209|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 249573 (244K) [application/java-archive]\n",
            "Saving to: spark-xml_2.11-0.9.0.jar\n",
            "\n",
            "\rspark-xml_2.11-0.9.   0%[                    ]       0  --.-KB/s               \rspark-xml_2.11-0.9. 100%[===================>] 243.72K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-07-28 00:51:15 (9.99 MB/s) - spark-xml_2.11-0.9.0.jar saved [249573/249573]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !ls\n",
        "!mv ./spark-xml_2.11-0.9.0.jar ../content/spark-3.2.2-bin-hadoop3.2/jars "
      ],
      "metadata": {
        "id": "mdjRbHUeO4Mg",
        "outputId": "6d1d0075-c2f6-45dd-984a-1c8e6a05e501",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat './spark-xml_2.11-0.9.0.jar': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "7clCmdK4PYOT",
        "outputId": "4762b770-6e6e-4335-8ba9-75ada0670901",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "clean_log_reviews.csv\tspark-3.2.2-bin-hadoop3.2\n",
            "clean_movie_revies.csv\tspark-3.2.2-bin-hadoop3.2.tgz\n",
            "log_reviews.csv\t\tspark-xml_2.11-0.9.0.jar\n",
            "movie_review.csv\tuser_purchase.csv\n",
            "sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "sparkk =  SparkSession.builder \\\n",
        "    .appName('ayayayay') \\\n",
        "    .master('mau') \\\n",
        "    .config(\"spark.jars.packages\", \"com.databricks:spark-xml_2.11-0.9.0.jar\") \\\n",
        "    .getOrCreate()\n",
        "customSchema = StructType([\n",
        "    StructField(\"logDate\", StringType(), True),\n",
        "    StructField(\"device\", StringType(), True),\n",
        "    StructField(\"location\", StringType(), True),\n",
        "    StructField(\"os\", StringType(), True),\n",
        "    StructField(\"ipAddress\", StringType(), True),\n",
        "    StructField(\"phoneNumber\", StringType(), True)])\n",
        "\n"
      ],
      "metadata": {
        "id": "yBLNs6JxQO0N"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!$SPARK_HOME/bin/spark-shell --packages com.databricks:spark-xml_2.11-0.9.0"
      ],
      "metadata": {
        "id": "zwFol1INUOOg",
        "outputId": "c4d60a99-695c-4c1a-b967-dd617d949b90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: /content/spark-3.2.2-bin-hadoop3.2: Is a directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "#os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.2-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession \n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "\n",
        "spark = SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"Our First Spark example\") \\\n",
        "       .getOrCreate()\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "id": "DluIdNEITvdF",
        "outputId": "28014649-24b7-4a67-9368-014c9bac1398",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f4290bff510>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://e49265cc8fdb:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Our First Spark example</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"Our First Spark example\") \\\n",
        "       .config(\"spark.jars.packages\", \"com.databricks:spark-xml_2.11-0.9.0\")\\\n",
        "       .getOrCreate()\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "id": "DDFqafGVUS5c",
        "outputId": "034375b8-d164-4fc1-b166-4b126a593b02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f4290bff510>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://e49265cc8fdb:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Our First Spark example</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log = spark.read.format(\"com.databricks.spark.xml\").\\\n",
        "  option(\"rootTag\",\"reviewlog\").\\\n",
        "  option(\"rowTag\",\"log\").\\\n",
        "  load('./log_reviewa.csv', schema = customSchema)"
      ],
      "metadata": {
        "id": "peqX562pQOwi",
        "outputId": "fbe17c33-d505-43e0-f659-c7e06605d527",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 879
        }
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-101-5498379ad795>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"com.databricks.spark.xml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rootTag\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"reviewlog\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rowTag\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"log\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./log_reviewa.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustomSchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.2.2-bin-hadoop3.2/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.2-bin-hadoop3.2/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.2-bin-hadoop3.2/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o661.load.\n: java.lang.ClassNotFoundException: \nFailed to find data source: com.databricks.spark.xml. Please find packages at\nhttp://spark.apache.org/third-party-projects.html\n       \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedToFindDataSourceError(QueryExecutionErrors.scala:443)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:670)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:720)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)\n\tat sun.reflect.GeneratedMethodAccessor67.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: com.databricks.spark.xml.DefaultSource\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:656)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:656)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:656)\n\t... 14 more\n"
          ]
        }
      ]
    }
  ]
}